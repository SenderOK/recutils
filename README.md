# recutils
LSML2018 recommender utils

## Общее описание утилиты
Разработана утилита на языке программирования Java для обучения линейных моделей (линейная модель без парных взаимодействий, FM).

Поддерживаются квадратичная функция потерь для задач регрессии и логистическая функция потерь для задач бинарной классификации. Модель без парных взаимодействий обучается с помощью SGD, для FM реализовано обучение с помощью SGD и ALS. 

Для SGD поддеживается параллельное обучение (Hogwild!). 

Поддерживается holdout-валидация (10% от обучающей выборки), при обучении выводится средняя ошибка на трейне (а если используется holdout, то и на валидации) после каждой эпохи обучения. Эпоха = один пробег по обучающей выборке для SGD, обновление всех весов модели для ALS.

Полный список опций: https://github.com/SenderOK/recutils/blob/master/src/main/java/ru.recutils/cli/CommandLineArguments.java

Обучение реализовано на основе следующих статей:

1) https://www.ismll.uni-hildesheim.de/pub/pdfs/Rendle_et_al2011-Context_Aware.pdf - правда, здесь в коде основной процедуры LearnALS допущено 2 опечатки: на строке 16 забыта звёздочка в присваиваемой переменной, на строке 23 последнее домножение должно быть не на x, а на h.

2) https://www.csie.ntu.edu.tw/~b97053/paper/Factorization%20Machines%20with%20libFM.pdf

3) https://people.eecs.berkeley.edu/~brecht/papers/hogwildTR.pdf - Hogwild!

Утилита работает с файлами в [формате vowpal wabbit](https://github.com/JohnLangford/vowpal_wabbit/wiki/Input-format), в настоящее время без полей *tag*, *base* и неймспейсов. 

Для управления размером модели применяется hashing trick (опция *-hb/--hashing-bits*), для хеширования используется заданное число последних битов MurmurHash3. Модели сохраняются с помощью стандатного метода сериализации в Java (интерфейс Serializable).

Для тестирования некоторых процедур реализованы юнит-тесты (JUnit).

### SGD
Для SGD реализовано онлайн-обучение, в начале каждой эпохи файл с обучающей выборкой открывается заново, объекты вычитываются из файла по одному и происходит обновление весов. Это может быть сделано в несколько потоков, что даёт значительное ускорение при обучении. 

Технически это реализовано с помощью параллельных потоков Java (интерфейс Stream), которые можно использовать для ввода (стандартный пакет java.nio.file.Files), атомарных счётчиков и ConcurrentHashMap для хранения весов.

Использован sgd c постоянным шагом.

### ALS
В отличие от SGD, ALS by design является последовательным. Реализован вариант с вычитыванием всего набора данных в память, чтобы для каждого фактора знать, у каких объектов он есть и какой имеет вес. Альтернативным вариантом является предварительное транспонирование набора данных во внешней памяти, однако для проведённых бенчмарков это не критично. 

Достоинством ALS является отсутствие необходимости подбирать темп обучения, нужно подбирать только регуляризаторы для одиночных весов и эмбеддингов для парных взаимодействий.

## Бенчмарки

### MovieLens-20m
Датасет случайным образом был разбит на трейн-тест 80-20, далее производились сравнения.

На этом датасете сравнение производилось с аналогичным FM-SGD режимом `--rank` VowpalWabbit. Параметры были подобраны, исходя из приведённого разработчиками [примера](https://github.com/JohnLangford/vowpal_wabbit/wiki/Matrix-factorization-example) (найден более хороший learning rate). Также был произведена попытка сравниться с [режимом](https://github.com/JohnLangford/vowpal_wabbit/tree/master/demo/movielens) `--lrq` в VowpalWabbit, однако результаты оказались значительно ниже чем у режима `--rank`.

Для утилиты наилучшие параметры были подобраны путём анализа ошибки на holdout-валидации, для FM-ALS настраивались регуляризаторы, для REGRESSION и SGD - темп обучения (регуляризаторы там слабее влияют на результат).

Во всех методах с эмбеддингами использовалась фиксированная размерность 10.

| Метод         | N эпох | Параметры обучения  | Ошибка на трейне | Ошибка на holdout | Ошибка на тесте | Время обучения |
|:-------------:|:------:|:-------------------:|:----------------:|:-----------------:|:---------------:|:--------------:|
| Regression    |  50    | `-t 32`             | 0.7562           | 0.7653            | 0.77144         | 4 мин          |
| FM-ALS        |  50    | `-l2 1e-6 -l2e 1e-6`|                  |                   |                 |                |
| FM-SGD        | 100    | `-t 32 -r 0.002`    | 0.6090           | 0.6661            | 0.6777          | 10 мин         |
| VW            |  81    | `-l 0.005`          | 0.5285           | 0.6300            | 0.6323          | 24 мин         |

Для обучения в режимах Regression и FM-SGD используется мощь Hogwild на 32 процессорах, за счёт чего время обучения заметно ниже. Ожидаемо, простая модель линейной регрессии, предсказывающая рейтинг как сумму свободного члена, веса при пользователе и веса при фильме значительно проиграла другим методам, при этом обучается она очень быстро.

## Выводы
1) При разработке большое внимание уделялось прозрачности архитектуры, поддерживаемости кода, тестированию. Делать это на Java легко и приятно благодаря богатому набору библиотек, умной среде разработки, ясной объектно-ориентированной модели. Однако при проектировании была допущена ошибка: в изначальной архитектуре проекта для обучения SGD не был предусмотрен паралеллизм, предполагалось добавить его позднее. На практике это привело к необходимости почти полного переписывания процедур для ввода и обучения.

2) SGD just wants to work, этот метод достаточно легко настраивается и выдаёт адекватные результаты, сопоставимые с известными Open-source инструментами для обучения. Кроме того, он не потребляет много памяти, его можно эффективно распараллелить.

## Инструкция по развёртыванию
Перед началом обновляем список пакетов и устанавливаем Java 8 - это потребуется, чтобы запустить JAR-файл с утилитой. Для Unix/Linux команды:
```
sudo apt-get update
sudo apt-get install oracle-java8-installer
```

### Запуск готового JAR-файла
Уже собранный JAR-файл можно [скачать](https://yadi.sk/d/R8js3Y6k3VqNmr) и использовать, например, вывести справку:
```
java -jar recutils.jar --help
```

Удобно может быть также сделать alias:
```
alias recutils='java -jar /path/to/recutils.jar'
recutils --help
```

### Сборка из исходного кода для Unix/Linux
В проекте используется Java 8, система сборки Maven, её необходимо предустановить: 
```
sudo apt-get update
sudo apt-get install maven
```
Далее, копируем репозиторий и собираем JAR-файл (содержит в себе все зависимости):
```
git clone https://github.com/SenderOK/recutils.git
cd recutils
mvn clean package
cd target
java -jar recutils.jar --help
```
Последняя команда выводит справку, передавая в соответствии с ней параметры командной строки, производится запуск утилиты.
